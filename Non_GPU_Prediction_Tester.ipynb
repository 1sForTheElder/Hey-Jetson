{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common, File Based, and Math Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "from subprocess import check_output\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from glob import glob\n",
    "import random\n",
    "from random import sample\n",
    "import json\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Audio processing\n",
    "from scipy import signal\n",
    "from scipy.fftpack import dct\n",
    "import soundfile\n",
    "import json\n",
    "from python_speech_features import mfcc\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "\n",
    "# Neural Network\n",
    "import keras\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Flatten, Embedding, merge, Activation, GRUCell, LSTMCell,SimpleRNNCell\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Convolution1D, Conv1D, SimpleRNN, GRU, LSTM, CuDNNLSTM, CuDNNGRU, Conv2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ThresholdedReLU, ELU\n",
    "from keras.layers import LeakyReLU, PReLU, ThresholdedReLU, ELU\n",
    "from keras.layers import BatchNormalization, TimeDistributed, Bidirectional\n",
    "from keras.layers import activations, Wrapper\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.utils import np_utils\n",
    "from keras import constraints, initializers, regularizers\n",
    "from keras.engine.topology import Layer\n",
    "import keras.losses\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.engine import InputSpec\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.core.protobuf import saver_pb2\n",
    "from tensorflow.python.training import saver as saver_lib\n",
    "\n",
    "# Setting Random Seeds\n",
    "np.random.seed(95)\n",
    "RNG_SEED = 95\n",
    "\n",
    "# Suppressing some of Tensorflow's warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for shuffling data\n",
    "def shuffle_dataset(audio_paths, durations, texts):\n",
    "    p = np.random.permutation(len(audio_paths))\n",
    "    audio_paths = [audio_paths[i] for i in p] \n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "# Function for sorting data by duration\n",
    "def sort_dataset(audio_paths, durations, texts):\n",
    "    p = np.argsort(durations).tolist()\n",
    "    audio_paths = [audio_paths[i] for i in p]\n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "# Mapping each character that could be spoken at each time step\n",
    "char_map_str = \"\"\"\n",
    "' 0\n",
    "<SPACE> 1\n",
    "a 2\n",
    "b 3\n",
    "c 4\n",
    "d 5\n",
    "e 6\n",
    "f 7\n",
    "g 8\n",
    "h 9\n",
    "i 10\n",
    "j 11\n",
    "k 12\n",
    "l 13\n",
    "m 14\n",
    "n 15\n",
    "o 16\n",
    "p 17\n",
    "q 18\n",
    "r 19\n",
    "s 20\n",
    "t 21\n",
    "u 22\n",
    "v 23\n",
    "w 24\n",
    "x 25\n",
    "y 26\n",
    "z 27\n",
    "\"\"\"\n",
    "# This leaves \"blank\" character mapped to number 28\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "for line in char_map_str.strip().split('\\n'):\n",
    "    ch, index = line.split()\n",
    "    char_map[ch] = int(index)\n",
    "    index_map[int(index)+1] = ch\n",
    "index_map[2] = ' '\n",
    "\n",
    "# Function for converting text to an integer sequence\n",
    "def text_to_int_seq(text):\n",
    "    int_sequence = []\n",
    "    for c in text:\n",
    "        if c == ' ':\n",
    "            ch = char_map['<SPACE>']\n",
    "        else:\n",
    "            ch = char_map[c]\n",
    "        int_sequence.append(ch)\n",
    "    return int_sequence\n",
    "\n",
    "# Function for converting an integer sequence to text\n",
    "def int_seq_to_text(int_sequence):\n",
    "    text = []\n",
    "    for c in int_sequence:\n",
    "        ch = index_map[c]\n",
    "        text.append(ch)\n",
    "    return text\n",
    "# Function for calculating feature dimensions.\n",
    "def calc_feat_dim(window, max_freq):\n",
    "    return int(0.001 * window * max_freq) + 1\n",
    "\n",
    "class AudioGenerator():\n",
    "    def __init__(self, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
    "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
    "        sort_by_duration=False):\n",
    "        # Initializing variables\n",
    "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
    "        self.mfcc_dim = mfcc_dim\n",
    "        self.feats_mean = np.zeros((self.feat_dim,))\n",
    "        self.feats_std = np.ones((self.feat_dim,))\n",
    "        self.rng = random.Random(RNG_SEED)\n",
    "        if desc_file is not None:\n",
    "            self.load_metadata_from_desc_file(desc_file)\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.max_freq = max_freq\n",
    "        self.cur_train_index = 0\n",
    "        self.cur_valid_index = 0\n",
    "        self.cur_test_index = 0\n",
    "        self.max_duration=max_duration\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.spectrogram = spectrogram\n",
    "        self.sort_by_duration = sort_by_duration\n",
    "\n",
    "    def get_batch(self, partition):\n",
    "    # Obtain a batch of audio files\n",
    "        if partition == 'train':\n",
    "            audio_paths = self.train_audio_paths\n",
    "            cur_index = self.cur_train_index\n",
    "            texts = self.train_texts\n",
    "        elif partition == 'valid':\n",
    "            audio_paths = self.valid_audio_paths\n",
    "            cur_index = self.cur_valid_index\n",
    "            texts = self.valid_texts\n",
    "        elif partition == 'test':\n",
    "            audio_paths = self.test_audio_paths\n",
    "            cur_index = self.test_valid_index\n",
    "            texts = self.test_texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/val\")\n",
    "\n",
    "        features = [self.normalize(self.featurize(a)) for a in \n",
    "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
    "\n",
    "        # Calculate size\n",
    "        max_length = max([features[i].shape[0] \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        max_string_length = max([len(texts[cur_index+i]) \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        \n",
    "        # Initialize arrays\n",
    "        X_data = np.zeros([self.minibatch_size, max_length, \n",
    "            self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
    "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
    "        input_length = np.zeros([self.minibatch_size, 1])\n",
    "        label_length = np.zeros([self.minibatch_size, 1])\n",
    "        \n",
    "        for i in range(0, self.minibatch_size):\n",
    "            # Calculate input_length\n",
    "            feat = features[i]\n",
    "            input_length[i] = feat.shape[0]\n",
    "            X_data[i, :feat.shape[0], :] = feat\n",
    "\n",
    "            # Calculate label_length\n",
    "            label = np.array(text_to_int_seq(texts[cur_index+i])) \n",
    "            labels[i, :len(label)] = label\n",
    "            label_length[i] = len(label)\n",
    "\n",
    "        # Output arrays\n",
    "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
    "        inputs = {'the_input': X_data, \n",
    "                  'the_labels': labels, \n",
    "                  'input_length': input_length, \n",
    "                  'label_length': label_length \n",
    "                 }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def shuffle_dataset_by_partition(self, partition):\n",
    "    # Shuffle the data\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_dataset(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_dataset(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/val\")\n",
    "\n",
    "    def sort_dataset_by_duration(self, partition):\n",
    "    # Sort the data\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = sort_dataset(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_dataset(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/val\")\n",
    "\n",
    "    def next_train(self):\n",
    "    # Get a batch of training data\n",
    "        while True:\n",
    "            ret = self.get_batch('train')\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
    "                self.cur_train_index = 0\n",
    "                self.shuffle_dataset_by_partition('train')\n",
    "            yield ret    \n",
    "\n",
    "    def next_valid(self):\n",
    "    # Get a batch of validation data\n",
    "        while True:\n",
    "            ret = self.get_batch('valid')\n",
    "            self.cur_valid_index += self.minibatch_size\n",
    "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
    "                self.cur_valid_index = 0\n",
    "                self.shuffle_dataset_by_partition('valid')\n",
    "            yield ret\n",
    "\n",
    "    def next_test(self):\n",
    "    # Get a batch of testing data\n",
    "        while True:\n",
    "            ret = self.get_batch('test')\n",
    "            self.cur_test_index += self.minibatch_size\n",
    "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
    "                self.cur_test_index = 0\n",
    "            yield ret\n",
    "            \n",
    "    # Load datasets\n",
    "    def load_train_data(self, desc_file='train_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
    "        self.fit_train()\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_dataset_by_duration('train')\n",
    "\n",
    "    def load_validation_data(self, desc_file='valid_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_dataset_by_duration('valid')\n",
    "\n",
    "    def load_test_data(self, desc_file='test_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
    "    \n",
    "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
    "    # Get metadata from json corpus\n",
    "        audio_paths, durations, texts = [], [], []\n",
    "        with open(desc_file) as json_line_file:\n",
    "            for line_num, json_line in enumerate(json_line_file):\n",
    "                try:\n",
    "                    spec = json.loads(json_line)\n",
    "                    if float(spec['duration']) > self.max_duration:\n",
    "                        continue\n",
    "                    audio_paths.append(spec['key'])\n",
    "                    durations.append(float(spec['duration']))\n",
    "                    texts.append(spec['text'])\n",
    "                except Exception as e:\n",
    "                    print('Error reading line #{}: {}'\n",
    "                                .format(line_num, json_line))\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths = audio_paths\n",
    "            self.train_durations = durations\n",
    "            self.train_texts = texts\n",
    "        elif partition == 'validation':\n",
    "            self.valid_audio_paths = audio_paths\n",
    "            self.valid_durations = durations\n",
    "            self.valid_texts = texts\n",
    "        elif partition == 'test':\n",
    "            self.test_audio_paths = audio_paths\n",
    "            self.test_durations = durations\n",
    "            self.test_texts = texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "             \"Must be train/val/test\")\n",
    "            \n",
    "    def fit_train(self, k_samples=100):\n",
    "    # Estimate descriptive stats for training set based on sample of 100\n",
    "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
    "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
    "        feats = [self.featurize(s) for s in samples]\n",
    "        feats = np.vstack(feats)\n",
    "        self.feats_mean = np.mean(feats, axis=0)\n",
    "        self.feats_std = np.std(feats, axis=0)\n",
    "        \n",
    "    def featurize(self, audio_clip):\n",
    "    # Create features from data, either spectrogram or mfcc\n",
    "        if self.spectrogram:\n",
    "            return spectrogram_from_file(\n",
    "                audio_clip, step=self.step, window=self.window,\n",
    "                max_freq=self.max_freq)\n",
    "        else:\n",
    "            (rate, sig) = wav.read(audio_clip)\n",
    "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
    "\n",
    "    def normalize(self, feature, eps=1e-14):\n",
    "    # Scale the data\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "    \n",
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "# Create a spectrogram from audio signals\n",
    "    assert not np.iscomplexobj(samples), \"You shall not pass in complex numbers\"\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)  \n",
    "    scale = window_norm * sample_rate\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "    # Reshape to include the overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "    # Window stride sanity check\n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "    # Broadcast window, and then compute fft over columns and square mod\n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "    # Scale 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "    return x, freqs\n",
    "\n",
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "# Calculate log(linear spectrogram) from FFT energy\n",
    "    with soundfile.SoundFile(filename) as sound_file:\n",
    "        audio = sound_file.read(dtype='float32')\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if audio.ndim >= 2:\n",
    "            audio = np.mean(audio, 1)\n",
    "        if max_freq is None:\n",
    "            max_freq = sample_rate / 2\n",
    "        if max_freq > sample_rate / 2:\n",
    "            raise ValueError(\"max_freq can not be > than 0.5 of \"\n",
    "                             \" sample rate\")\n",
    "        if step > window:\n",
    "            raise ValueError(\"step size can not be > than window size\")\n",
    "        hop_length = int(0.001 * step * sample_rate)\n",
    "        fft_length = int(0.001 * window * sample_rate)\n",
    "        pxx, freqs = spectrogram(\n",
    "            audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "            hop_length=hop_length)\n",
    "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))\n",
    "\n",
    "def vis_train_features(index):\n",
    "# Function for visualizing a single audio file based on index chosen\n",
    "    # Get spectrogram\n",
    "    audio_gen = AudioGenerator(spectrogram=True)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
    "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # Get mfcc\n",
    "    audio_gen = AudioGenerator(spectrogram=False)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # Obtain text label\n",
    "    vis_text = audio_gen.train_texts[index]\n",
    "    # Obtain raw audio\n",
    "    vis_raw_audio, _ = librosa.load(vis_audio_path)\n",
    "    # Print total number of training examples\n",
    "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
    "    # Return labels for plotting\n",
    "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CTC loss function (discussed below)\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def add_ctc_loss(input_to_softmax):\n",
    "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
    "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
    "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model(\n",
    "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
    "        outputs=loss_out)\n",
    "    return model\n",
    "\n",
    "# Functions for modifying CNN layers for sequence problems \n",
    "def conv_output_length(input_length, filter_size, border_mode, stride,\n",
    "                       dilation=1):\n",
    "# Compute the length of conv output seq after 1D convolution across time\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert border_mode in {'same', 'valid', 'causal'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if border_mode == 'same':\n",
    "        output_length = input_length\n",
    "    elif border_mode == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    elif border_mode == 'causal':\n",
    "        output_length = input_length\n",
    "    return (output_length + stride - 1) // stride\n",
    "\n",
    "def cnn_output_length(input_length, filter_size, border_mode, stride,\n",
    "                       dilation=1):\n",
    "# Compute the length of cnn output seq after 1D convolution across time\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert border_mode in {'same', 'valid', 'causal'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if border_mode == 'same':\n",
    "        output_length = input_length\n",
    "    elif border_mode == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    elif border_mode == 'causal':\n",
    "        output_length = input_length\n",
    "    return (output_length + stride - 1) // stride\n",
    "\n",
    "def train_model(input_to_softmax, \n",
    "                pickle_path,\n",
    "                save_model_path,\n",
    "                train_json='train_corpus.json',\n",
    "                valid_json='valid_corpus.json',\n",
    "                minibatch_size=20,\n",
    "                spectrogram=True,\n",
    "                mfcc_dim=13,\n",
    "                optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, clipnorm=1, clipvalue=0.5),\n",
    "                epochs=30,\n",
    "                verbose=1,\n",
    "                sort_by_duration=False,\n",
    "                max_duration=10.0):\n",
    "    \n",
    "    # Obtain batches of data\n",
    "    audio_gen = AudioGenerator(minibatch_size=minibatch_size, \n",
    "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
    "        sort_by_duration=sort_by_duration)\n",
    "    # Load the datasets\n",
    "    audio_gen.load_train_data(train_json)\n",
    "    audio_gen.load_validation_data(valid_json)\n",
    "    # Calculate steps per epoch\n",
    "    num_train_examples=len(audio_gen.train_audio_paths)\n",
    "    steps_per_epoch = num_train_examples//minibatch_size\n",
    "    # Calculate validation steps\n",
    "    num_valid_samples = len(audio_gen.valid_audio_paths) \n",
    "    validation_steps = num_valid_samples//minibatch_size    \n",
    "    # Add custom CTC loss function to the nn\n",
    "    model = add_ctc_loss(input_to_softmax)\n",
    "    # Dummy lambda function for loss since CTC loss is implemented above\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "    # Make  initial results/ directory for saving model pickles\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    # Add checkpoints\n",
    "    checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)\n",
    "    # Fit/train model\n",
    "    hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps,\n",
    "        callbacks=[checkpointer], verbose=verbose)\n",
    "    # Save model loss\n",
    "    with open('results/'+pickle_path, 'wb') as f:\n",
    "        pickle.dump(hist.history, f)\n",
    "\n",
    "# Creating a TensorFlow session\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "#set_session(tf.Session(config=config))\n",
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_deep_brnn_dropout_model(input_dim, filters, activation, kernel_size, conv_stride,\n",
    "    conv_border_mode, recur_layers, units, output_dim=29):\n",
    "    # Input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Convolutional layer\n",
    "    conv_1d = Conv1D(filters, kernel_size, \n",
    "                     strides=conv_stride, \n",
    "                     padding=conv_border_mode,\n",
    "                     activation=activation,\n",
    "                     name='conv1d')(input_data)\n",
    "    # Batch normalization\n",
    "    bn_cnn = BatchNormalization()(conv_1d)\n",
    "    # Bidirectional recurrent layer\n",
    "    brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, implementation=2, recurrent_dropout=0.01, name='brnn'))(bn_cnn)\n",
    "    # Batch normalization \n",
    "    bn_rnn = BatchNormalization()(brnn)\n",
    "    # Loop for additional layers\n",
    "    for i in range(recur_layers - 1):\n",
    "        name = 'brnn_' + str(i + 1)\n",
    "        brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, implementation=2, name=name))(bn_rnn)\n",
    "        bn_rnn = BatchNormalization()(brnn)\n",
    "    # TimeDistributed Dense layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specifying the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)\n",
    "    return model\n",
    "\n",
    "model_6 = cnn_deep_brnn_dropout_model(input_dim=161, # 161 for Spectrogram/13 for MFCC\n",
    "                                      filters=200,\n",
    "                                      activation='relu',\n",
    "                                      kernel_size=11, \n",
    "                                      conv_stride=2,\n",
    "                                      conv_border_mode='valid',\n",
    "                                      recur_layers=2,\n",
    "                                      units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_deep_brnn_dilated_model(input_dim, filters, activation, kernel_size, conv_stride,\n",
    "    conv_border_mode, recur_layers, dilation_rate, units, conv_layers, output_dim=29):\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    conv_1d = Conv1D(filters, kernel_size, \n",
    "                     strides=conv_stride, \n",
    "                     padding=conv_border_mode,\n",
    "                     activation=activation,\n",
    "                     name='conv1d')(input_data)\n",
    "    # Batch normalization\n",
    "    bn_cnn = BatchNormalization()(conv_1d)\n",
    "    for i in range(conv_layers - 1):\n",
    "        conv_1d = Conv1D(filters, kernel_size,\n",
    "                         padding='causal',\n",
    "                         activation='relu',\n",
    "                         dilation_rate=2**i,\n",
    "                         name=\"conv_1d_\"+str(i))(bn_cnn)\n",
    "        bn_cnn = BatchNormalization()(conv_1d)\n",
    "   # Bidirectional recurrent layer\n",
    "    brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, implementation=2, name='brnn'))(bn_cnn)\n",
    "    # Batch normalization \n",
    "    bn_rnn = BatchNormalization()(brnn)\n",
    "    # Loop for additional layers\n",
    "    for i in range(recur_layers - 1):\n",
    "        name = 'brnn_' + str(i + 1)\n",
    "        brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, implementation=2, name=name))(bn_rnn)\n",
    "        bn_rnn = BatchNormalization()(brnn)\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, 'causal', 1)\n",
    "    return model\n",
    "\n",
    "model_7 = cnn_deep_brnn_dilated_model(input_dim=161, # 161 for Spectrogram/13 for MFCC\n",
    "                                      filters=200,\n",
    "                                      activation='relu',\n",
    "                                      kernel_size=11, \n",
    "                                      conv_stride=1,\n",
    "                                      conv_border_mode='causal',\n",
    "                                      recur_layers=2,\n",
    "                                      conv_layers=2,\n",
    "                                      dilation_rate=2,\n",
    "                                      units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model(input_dim, filters, activation, dilation_rate, kernel_size, conv_stride,\n",
    "    conv_border_mode, recur_layers, units, conv_layers, output_dim=29):\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # First Convolution Layer\n",
    "    conv_1d = Conv1D(filters, kernel_size, \n",
    "                     strides=conv_stride, \n",
    "                     padding=conv_border_mode,\n",
    "                     activation=activation,\n",
    "                     name='conv1d')(input_data)\n",
    "    # Batch Normalization\n",
    "    bn_cnn = BatchNormalization()(conv_1d)\n",
    "    # Dilated Convolutions\n",
    "    for i in range(conv_layers - 1):\n",
    "        conv_1d = Conv1D(filters, kernel_size,\n",
    "                         padding='causal',\n",
    "                         activation='relu',\n",
    "                         dilation_rate=2**i,\n",
    "                         name=\"conv_1d_\"+str(i))(bn_cnn)\n",
    "        bn_cnn = BatchNormalization()(conv_1d)\n",
    "   # Bidirectional recurrent layer\n",
    "    brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, implementation=2, recurrent_dropout=0.1, name='brnn'))(bn_cnn)\n",
    "    # Batch normalization \n",
    "    bn_rnn = BatchNormalization()(brnn)\n",
    "    # Loop for additional recurrent layers\n",
    "    for i in range(recur_layers - 1):\n",
    "        name = 'brnn_' + str(i + 1)\n",
    "        brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, name=name))(bn_rnn)\n",
    "        bn_rnn = BatchNormalization()(brnn)\n",
    "    # Time Dense\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, 'causal', 1)\n",
    "    return model\n",
    "\n",
    "model_8 = keras_model(input_dim=161, # 161 for Spectrogram/13 for MFCC\n",
    "                      filters=200,\n",
    "                      activation='relu',\n",
    "                      kernel_size=11, \n",
    "                      conv_stride=1,\n",
    "                      conv_border_mode='causal',\n",
    "                      recur_layers=7,\n",
    "                      conv_layers=3,\n",
    "                      dilation_rate=2,\n",
    "                      units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(index, partition, input_to_softmax, model_path):\n",
    "\n",
    "    # Load the train and test data\n",
    "    data_gen = AudioGenerator(spectrogram = spectrogram)\n",
    "    data_gen.load_validation_data()\n",
    "    data_gen.load_test_data()\n",
    "    \n",
    "    # Obtain ground truth transcriptions and audio features \n",
    "    if partition == 'validation':\n",
    "        transcription = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'test':\n",
    "        transcription = data_gen.test_texts[index]\n",
    "        audio_path = data_gen.test_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"test\", or \"validation\"')\n",
    "        \n",
    "    # Obtain predictions\n",
    "    input_to_softmax.load_weights(model_path)\n",
    "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
    "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length)[0][0])+1).flatten().tolist()\n",
    "    \n",
    "    # Display ground truth transcription\n",
    "    truth_transcription = 'True transcription: ' + transcription\n",
    "    return truth_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(index, partition, input_to_softmax, model_path):\n",
    "\n",
    "    # Load the train and test data\n",
    "    data_gen = AudioGenerator(spectrogram = spectrogram)\n",
    "    data_gen.load_validation_data()\n",
    "    data_gen.load_test_data()\n",
    "    \n",
    "    # Obtain ground truth transcriptions and audio features \n",
    "    if partition == 'validation':\n",
    "        transcription = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'test':\n",
    "        transcription = data_gen.test_texts[index]\n",
    "        audio_path = data_gen.test_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"test\", or \"validation\"')\n",
    "        \n",
    "    # Obtain predictions\n",
    "    input_to_softmax.load_weights(model_path)\n",
    "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
    "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length)[0][0])+1).flatten().tolist()\n",
    "    \n",
    "    # Display predicted transcripted.\n",
    "    predicted_transcription = 'Predicted transcription: ' + ''.join(int_seq_to_text(pred_ints))\n",
    "\n",
    "    return predicted_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True transcription: then you can ask him questions on the catechism dedalus'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ground_truth(index=10, partition='test', input_to_softmax=model_7, model_path='./results/model_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predicted transcription: mfxjfjxzjzjzjzxzjxzjzxzxzxzjxzxzxzxzxczjzx xzczxzxzxcxjxzxzxzxcjzxzxzxcxzxzxc'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(index=10, partition='test', input_to_softmax=model_7, model_path='./results/model_7.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
